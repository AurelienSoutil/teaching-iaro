{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamiques multiagents\n",
    "\n",
    "> Rappel: si la visualisation (en particulier des commandes LaTeX est défaillante), ouvrir le même fichier dans le viewer jupyter: \n",
    "\n",
    "Reprenons les stratégies envisagées dans le cadre du dilemme du prisonnier. \n",
    "On voit que l'on peut les classer selon deux critères: \n",
    "\n",
    "* le **niveau d'information** requis par la règle: on parle d'une règle de niveau $k$ lorsque les $k$ derniers coups de l'adversaire sont requis. \n",
    "* le fait que la règle soit **déterministe ou stochastique**\n",
    "\n",
    "On peut parler d'apprentissage dès qu'une règle utilise la connaissance des coups joués par les autres joueurs, autrement dit dès qu'elle est de niveau $k\\geq 1$. \n",
    "\n",
    "> Pouvez-vous caractériser selon ces critères les règles random, tit-for-tat, toujours trahir...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Meilleures réponses avec plusieurs agents\n",
    "\n",
    "On a vu dans les cours précédents la notion de meilleure réponse. \n",
    "Remarquons tout d'abord que cette notion s'étend tout à fait naturellement à plus de deux agents. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque le système multiagent comporte plus de deux agents, on peut construire de manière similaire les tables de meilleures réponses, mais selon les possibilités de stratégies de **tous** les autres agents. \n",
    "\n",
    "| BR(x)  | $x_1$  | $x_2$ | $x_3$ |\n",
    "|------|------|------|------|\n",
    "| acheter  | emprunter| emprunter  | emprunter |\n",
    "| emprunter  | acheter| acheter  | emprunter |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Problème:** la table de meilleure réponse grandit exponentiellement avec le nombre d'agents. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toutefois, dans de nombreuses situations, les actions des agents n'ont que des conséquences locales. On peut donc limiter la représentation des meilleures réponses aux agents concernés. \n",
    "\n",
    "Imaginons par exemple que Riri, Fifi, et Loulou habitent dans une rue: Riri au début de la rue, Fifi au milieu de la rue, et Loulou à la fin de la rue. Les trois amis se demandent s'ils doivent acheter une tondeuse à gazon ou plutôt l'emprunter à leur voisin. Cette décision dépend du choix de leur voisin. Riri est voisin de Fifi, Fifi est voisin de Riri et Loulou, et Loulou n'est voisin que de Riri. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bestResponse(s,agent):\n",
    "    if agent == 'riri' or agent=='loulou':\n",
    "        if s['fifi']=='achete':\n",
    "            return 'emprunte'\n",
    "        else:\n",
    "            return 'achete'\n",
    "    elif agent == 'fifi':\n",
    "        if s['riri']=='emprunte' and s['loulou']=='emprunte':\n",
    "            return 'achete'\n",
    "        else: \n",
    "            return 'emprunte'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamiques de meilleures réponses\n",
    "On peut sur cette base étudier le processus dynamique qui résulte du fait que les agents jouent itérativement des meilleures réponses. \n",
    "\n",
    "De manière générale, on peut parler **partial best-response**: une partie de la population d'agents joue une meilleure réponse à l'état courant. On a donc deux cas limites: \n",
    "\n",
    "* la situation où **un seul agent** est considéré à chaque fois: les actions sont donc modifiée \n",
    "* la situation où **tous les agents** modifient leurs stratégies simultanément à chaque fois, en réponse à l'observation de l'état courant. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la version \"agent unique\", l'algorithme de dynamique de meilleure réponse prend donc la simple forme suivante: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loulou': 'achete', 'riri': 'achete', 'fifi': 'achete'}\n",
      "{'loulou': 'achete', 'riri': 'emprunte', 'fifi': 'achete'}\n",
      "{'loulou': 'achete', 'riri': 'emprunte', 'fifi': 'emprunte'}\n",
      "{'loulou': 'achete', 'riri': 'emprunte', 'fifi': 'emprunte'}\n",
      "{'loulou': 'achete', 'riri': 'achete', 'fifi': 'emprunte'}\n",
      "{'loulou': 'achete', 'riri': 'achete', 'fifi': 'emprunte'}\n",
      "{'loulou': 'achete', 'riri': 'achete', 'fifi': 'emprunte'}\n",
      "{'loulou': 'achete', 'riri': 'achete', 'fifi': 'emprunte'}\n",
      "{'loulou': 'achete', 'riri': 'achete', 'fifi': 'emprunte'}\n",
      "{'loulou': 'achete', 'riri': 'achete', 'fifi': 'emprunte'}\n"
     ]
    }
   ],
   "source": [
    "agents = ['riri', 'fifi', 'loulou']\n",
    "s= {'riri': 'achete', 'fifi': 'achete', 'loulou': 'achete'} # vecteur de stratégies initiales\n",
    "equilibrium = False\n",
    "print (s)\n",
    "while not equilibrium:\n",
    "    equilibrium = True\n",
    "    for i in agents:\n",
    "        if s[i]!=bestResponse(s,i):\n",
    "            s[i]=bestResponse(s,i)\n",
    "            equilibrium=False\n",
    "        print (s)\n",
    "    \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Que se passe-t-il sur cet exemple si on considère la dynamique où tous les agents modifient simultanément leurs stratégies?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quelques propriétés simples des dynamiques de meilleures réponses: \n",
    "\n",
    "* Lorsqu'un équilibre est atteint, il s'agit bien d'un équilibre de Nash (par définition). \n",
    "* Les dynamiques de meilleures réponses cyclent nécessairement lorsqu'il n'y a pas d'équilibre de Nash\n",
    "* Les dynamiques de meilleures réponses peuvent cycler même si il existe des équilibres de Nash \n",
    "* Toutefois, sous certaines conditions, on peut montrer qu'elles convergent nécessairement vers des équilibres de Nash. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fictitious Play\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supposons maintenant que les agents apprennent plus spécifiquement le comportement des autres joueurs, et jouent la meilleure réponse à la prédiction qu'ils font du comportement des autres agents. \n",
    "Un des modèles les plus simples d'un tel type d'apprentissage a été proposé par Brown (1951) sous le nom de *fictitious play*. \n",
    "\n",
    "Il repose donc sur les phases suivantes: \n",
    "\n",
    "* prédiction des actions des autres joueurs\n",
    "* choix de la meilleure réponse à la stratégie des autres agents\n",
    "* observation des actions des autres joueurs, mise à jour de la prédiction\n",
    "\n",
    "**(Standard) Fictitious Play**: \n",
    "Nous présentons ici le modèle dans un contexte où seuls deux agents sont en interaction. (Le modèle s'étend à plus d'agents, mais il faut alors faire des choix sur la manière de combiner les probabilités assigné\n",
    "\n",
    "Chaque joueur tient à jour un modèle simple des autres joueurs (la fréquence de sélection des actions) et joue sa meilleure réponse à ce modèle probabiliste. \n",
    "Autrement dit: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Exemple: \n",
    "\n",
    "Supposons que le joueur \\textcolor{blue}{bleu} joue aveu ou silence une fois sur deux. \n",
    "\n",
    "La table suivante donne les gains pour le joueur R. \n",
    "\n",
    "| | c | d |\n",
    "| ----|---- | ---- |\n",
    "|**a** | 8 | 10 |\n",
    "|**b** | 10 | 0 |\n",
    "\n",
    "\n",
    "Supposons que la stratégie de B soit de jouer alternativement c et d. \n",
    "Il faut également que l'agent fasse une prédiction *a priori* sur l'action initiale de l'agent \n",
    "\n",
    "\n",
    "| | BR(R) | coup B | #c | #d | P(c) | P(d) |\n",
    "| ----|---- | ---- | ---- | ---- | ---- |\n",
    "|**a** | 8 | 10 |\n",
    "|**b** | 10 | 0 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autres techniques d'apprentissage\n",
    "\n",
    "Considérons le problème "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
